{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from nd2reader import ND2Reader\n",
    "import napari\n",
    "import bigfish\n",
    "import bigfish.stack as stack\n",
    "import bigfish.detection as detection\n",
    "import bigfish.multistack as multistack\n",
    "import bigfish.plot as plot\n",
    "import time\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "from skimage import io\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = \"./\" ##Input Directory with the ND2 files\n",
    "dir3 = dir1+'TiffFiles'\n",
    "\n",
    "if not os.path.isdir(dir3):\n",
    "    os.mkdir(dir3)\n",
    "\n",
    "files=[]\n",
    "files += [each for each in os.listdir(dir1) if each.endswith('.nd2')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "string = 'Bleach'\n",
    "mask = np.array([string not in s for s in files])\n",
    "\n",
    "files = files[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files as OME Tiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All images are converte to .tif files and saved in the TiffFiles folder\n",
    "- Unclear yet how to change the LUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nis2pyr.convertor import convert_nd2_to_pyramidal_ome_tiff\n",
    "\n",
    "for fil in files:\n",
    "    print(dir1+fil)\n",
    "    convert_nd2_to_pyramidal_ome_tiff(dir1+fil, dir3+'/'+fil.split(\".\")[0]+'.tif',max_levels=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Intensity Projection and Image Distribution according to FOV \n",
    "So all images corresponding to the same FOV but across different cycles are saved in one folder \n",
    "- each image for one round is split by its channels and one channel image is saved\n",
    "- still need to figure out how to manage when there are multiple channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir4 = dir3+'/Max_Projections/'\n",
    "dir50 = dir4+'FOVs/'\n",
    "if not os.path.isdir(dir50):\n",
    "    os.mkdir(dir50)\n",
    "\n",
    "files=[]\n",
    "files += [each for each in os.listdir(dir3) if each.endswith('.tif')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "\n",
    "if not os.path.isdir(dir4):\n",
    "    os.mkdir(dir4)\n",
    "\n",
    "def maximum_intensity_projection(image_stack):\n",
    "    # Calculate the maximum intensity projection along the Z-axis (channels are in the first dimension)\n",
    "    mip = np.max(image_stack, axis=0)\n",
    "    return mip\n",
    "\n",
    "for fil in files:\n",
    "    #print(fil)\n",
    "    ome_tiff_path = dir3+\"/\"+fil\n",
    "    image_stack = tiff.imread(ome_tiff_path)\n",
    "    mip = maximum_intensity_projection(image_stack)\n",
    "    mip_path = dir4+\"MAX_\"+fil.split(\".\")[0]+\".tif\"\n",
    "    tiff.imsave(mip_path, mip)\n",
    "    FOV = fil.split(\"_\")\n",
    "    dir5 = dir50+\"/\"+FOV[1].split(\".\")[0]+\"/\"\n",
    "    if not os.path.isdir(dir5):\n",
    "        os.mkdir(dir5)\n",
    "    for chan in range(0,mip.shape[0]):\n",
    "        image = mip[chan,:,:]\n",
    "        savename =  dir5+FOV[0]+\"_channel_\"+str(chan+1)+\".tif\"\n",
    "        tiff.imsave(savename, image)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Files combining images from single channel and single FOV across all cycles\n",
    "- This helps in easy visualization of the data and can also be used for data analysis? \n",
    "- the files are saved in the Max_Projection/Combined_Files/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [name for name in os.listdir(dir4) if os.path.isdir(os.path.join(dir4, name))]\n",
    "directories.sort()\n",
    "\n",
    "savedir = dir4+\"/\"+\"Combined_Files/\"\n",
    "if not os.path.isdir(savedir):\n",
    "    os.mkdir(savedir)\n",
    "\n",
    "for dir in directories:\n",
    "    fils=[]\n",
    "    dirs = dir4+\"/\"+dir\n",
    "    fils += [each for each in os.listdir(dirs) if each.endswith('.tif')]\n",
    "    fils = np.array(fils)\n",
    "    fils.sort()\n",
    "    chan = np.char.split(fils,sep=\"_\")\n",
    "    chan = pd.DataFrame(np.stack(chan,axis=0))[2]\n",
    "    chans = pd.unique(chan)\n",
    "    for c in chans:\n",
    "        index = chan==c\n",
    "        multifils = fils[index]\n",
    "        multichannel_image = np.zeros((len(multifils),image.shape[0], image.shape[1]))\n",
    "        round=0\n",
    "        for m in multifils:\n",
    "            img = tiff.imread(dirs+\"/\"+m)\n",
    "            multichannel_image[round,:,:] = img\n",
    "            round=round+1\n",
    "        savename = savedir+dir+\"_channel_\"+c\n",
    "        tiff.imsave(savename, multichannel_image)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FISH Spot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detecting threshold within multiple FOVs for all rounds and channels\n",
    "- This could potentially be used to get a global threshold value\n",
    "- Alternatively can be used to get individual thresholds for each channel and round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir4 = dir3+'/Max_Projections/'\n",
    "dir50 = dir4+'FOVs/'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "voxelval = 110.3752759382\n",
    "radiusval = 250.0\n",
    "\n",
    "thresh1=18 ## Threshold for Channel 1 (generally Cy7)\n",
    "thresh2=18 ## Threshold for Channel 2 (generally Cy5)\n",
    "thresh3=18 ## Threshold for Channel 3 (generally Cy3B)\n",
    "\n",
    "#Number of Cycles for each channel\n",
    "chan1round = 8\n",
    "chan2round = 8\n",
    "chan3round = 8\n",
    "\n",
    "## Number of FOVs to use to detect thresholds\n",
    "nfovs = 15\n",
    "\n",
    "dir10 = dir4+\"/\"+\"Combined_Files/\"\n",
    "files=[]\n",
    "files += [each for each in os.listdir(dir10) if each.endswith('.tif')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "chan = np.char.split(files,sep=\"_\")\n",
    "chan = pd.DataFrame(np.stack(chan,axis=0))[2]\n",
    "chans = pd.unique(chan)\n",
    "\n",
    "ths1 = np.zeros((chan1round*nfovs, 1))\n",
    "ths2 = np.zeros((chan2round*nfovs+1, 1))\n",
    "ths3 = np.zeros((chan3round*nfovs, 1))\n",
    "\n",
    "spot_radius_px = detection.get_object_radius_pixel(\n",
    "                    voxel_size_nm=(voxelval, voxelval), \n",
    "                    object_radius_nm=(radiusval, radiusval), \n",
    "                    ndim=2)\n",
    "\n",
    "\n",
    "e = 0\n",
    "\n",
    "## Channel 1 - Cy7\n",
    "for fil in range(0,nfovs):\n",
    "\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    filename = files[fil*len(chans)]\n",
    "    img = tiff.imread(dir10+filename)\n",
    "    print(filename)\n",
    "    ### Detect spots\n",
    "    for t in range(0,img.shape[0]):\n",
    "        rna = img[t,:,:]    \n",
    "        # LoG filter\n",
    "        rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "        # local maximum detection\n",
    "        mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "        # thresholding\n",
    "        threshold = detection.automated_threshold_setting(rna_log, mask)\n",
    "        ths1[e] = threshold\n",
    "        e=e+1\n",
    "       \n",
    "print(\"Finished thresholding for channel 1 after %s seconds ---\" % (time.time() - start_time))       \n",
    "\n",
    "## Channel 2 - Cy5\n",
    "        \n",
    "e = 0\n",
    "for fil in range(0,nfovs):\n",
    "\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    filename = files[fil*len(chans)+1]\n",
    "    img = tiff.imread(dir10+filename)\n",
    "    print(filename)\n",
    "    ### Detect spots \n",
    "    for t in range(0,img.shape[0]):\n",
    "        rna = img[t,:,:]    \n",
    "        # LoG filter\n",
    "        rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "        # local maximum detection\n",
    "        mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "        # thresholding\n",
    "        threshold = detection.automated_threshold_setting(rna_log, mask)\n",
    "        ths2[e] = threshold\n",
    "        e=e+1\n",
    "        \n",
    "print(\"Finished thresholding for channel 2 after %s seconds ---\" % (time.time() - start_time))               \n",
    "\n",
    "## Channel 3 - Cy3B\n",
    "\n",
    "e = 0\n",
    "for fil in range(0,nfovs):\n",
    "\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    filename = files[fil*len(chans)+2]\n",
    "    img = tiff.imread(dir10+filename)\n",
    "    print(filename)\n",
    "    ### Detect spots\n",
    "    for t in range(0,img.shape[0]):\n",
    "        rna = img[t,:,:]    \n",
    "        # LoG filter\n",
    "        rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "        # local maximum detection\n",
    "        mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "        # thresholding\n",
    "        threshold = detection.automated_threshold_setting(rna_log, mask)\n",
    "        ths3[e] = threshold\n",
    "        e=e+1\n",
    "\n",
    "print(\"Finished thresholding for channel 3 after %s seconds ---\" % (time.time() - start_time))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying threshold to detect spots and clusters\n",
    "- spots are saved in the Spots folder as FOV##\\_Channel##\\_#.csv\n",
    "- spot clusters are save in the Spots folder as FOV##\\_Channel##\\_spotclusters_#.csv\n",
    "- clusters information is save in the Spots folder as FOV##\\_Channel##\\_clusters\\_#.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "voxelval = 110.3752759382 ##XY pixel size\n",
    "radiusval = 2*voxelval  ## size of spot approx - possible we could alter this based on channel??\n",
    "thresh1=2*np.median(ths1)+40 ## Threshold for Channel 1\n",
    "thresh2=2*np.median(ths2)+40 ## Threshold for Channel 2\n",
    "thresh3=2*np.median(ths3)+40 ## Threshold for Channel 3\n",
    "\n",
    "threshs = [thresh1,thresh2,thresh3]\n",
    "\n",
    "chan1round = 7\n",
    "chan2round = 7\n",
    "chan3round = 6\n",
    "\n",
    "dir10 = dir4+\"/\"+\"Combined_Files/\"\n",
    "files=[]\n",
    "files += [each for each in os.listdir(dir10) if each.endswith('.tif')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "chan = np.char.split(files,sep=\"_\")\n",
    "chan = pd.DataFrame(np.stack(chan,axis=0))[2]\n",
    "chans = pd.unique(chan)\n",
    "\n",
    "spot_radius_px = detection.get_object_radius_pixel(\n",
    "                    voxel_size_nm=(voxelval, voxelval), \n",
    "                    object_radius_nm=(radiusval, radiusval), \n",
    "                    ndim=2)\n",
    "\n",
    "nfovs = int(len(files)/len(chans))\n",
    "\n",
    "for fil in range(0,nfovs):\n",
    "\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    for cc in range(0,len(chans)-1):\n",
    "        print(\"Analysing Channel %s\"%(cc))\n",
    "        filename = files[fil*len(chans)+cc]\n",
    "        img = tiff.imread(dir10+filename)\n",
    "        savenamespots = filename.split(\".\")[0]+\".csv\"\n",
    "        savenamespotscl = filename.split(\".\")[0]+\"_spotclusters.csv\"\n",
    "        savenameclusters = filename.split(\".\")[0]+\"_clusters.csv\"\n",
    "        \n",
    "        sp = pd.DataFrame()\n",
    "        spcl = pd.DataFrame()\n",
    "        cl = pd.DataFrame()\n",
    "        \n",
    "        print(filename)\n",
    "        ### Detect spots\n",
    "        for t in range(0,img.shape[0]-1):\n",
    "            print(\"Analysing Round %s\"%(t+1))\n",
    "            rna = img[t+1,:,:]\n",
    "            # viewer.add_image(rna,rgb=False,\n",
    "            #                     name='FOV_'+str(fil)+\"_channel_\"+str(t+1))\n",
    "            # LoG filter\n",
    "            spots = detection.detect_spots(\n",
    "                                    images=rna, \n",
    "                                    return_threshold=False,\n",
    "                                    threshold=threshs[cc], \n",
    "                                    voxel_size=(voxelval, voxelval),  # in nanometer (one value per dimension zyx)\n",
    "                                    spot_radius=(radiusval, radiusval))  # in nanometer (one value per dimension zyx)\n",
    "\n",
    "            spots_post_decomposition, dense_regions, reference_spot = detection.decompose_dense(\n",
    "                                    image=np.uint16(rna), \n",
    "                                    spots=spots, \n",
    "                                    voxel_size=(voxelval, voxelval), \n",
    "                                    spot_radius=(radiusval, radiusval), \n",
    "                                    alpha=0.75,  # alpha impacts the number of spots per candidate region\n",
    "                                    beta=0.9,  # beta impacts the number of candidate regions to decompose\n",
    "                                    gamma=15)  # gamma the filtering step to denoise the image\n",
    "            \n",
    "            spots_post_clustering, clusters = detection.detect_clusters(\n",
    "                                    spots=spots_post_decomposition, \n",
    "                                    voxel_size=(int(voxelval), int(voxelval)), \n",
    "                                    radius=int(radiusval), \n",
    "                                    nb_min_spots=4)\n",
    "\n",
    "            spotspd = pd.DataFrame(spots)\n",
    "            spclpd = pd.DataFrame(spots_post_clustering)\n",
    "            clupd = pd.DataFrame(clusters)\n",
    "            \n",
    "            spotspd['round'] = t\n",
    "            spclpd['round'] = t\n",
    "            clupd['round'] = t\n",
    "                        \n",
    "            sp = pd.concat([sp,spotspd])\n",
    "            spcl = pd.concat([spcl,spclpd])\n",
    "            cl = pd.concat([cl,clupd])\n",
    "            \n",
    "        sp.columns = ['Y','X','round']\n",
    "        cl.columns = ['Y','X','nspots','index','round']\n",
    "        spcl.columns = ['Y','X','clusterindex','round']\n",
    "\n",
    "        sp.to_csv(savedir+savenamespots, index=False)\n",
    "        spcl.to_csv(savedir+savenamespotscl, index=False) \n",
    "        cl.to_csv(savedir+savenameclusters, index=False)     \n",
    "        \n",
    "    print(\"Finished thresholding for channel 1 image %s after %s seconds ---\" % (e,time.time() - start_time))  \n",
    "       \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Napari viewer\n",
    "- View all images in one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_square(limit):\n",
    "    answer = 0\n",
    "    while (answer+1)**2 < limit:\n",
    "        answer += 1\n",
    "    if answer**2 == limit:\n",
    "        return answer\n",
    "    else:\n",
    "        return answer+1\n",
    "\n",
    "viewer= napari.Viewer()\n",
    "#nfovs=2\n",
    "\n",
    "gap = 2800\n",
    "\n",
    "totalarea = nearest_square(len(files)/4)\n",
    "image=np.zeros((gap*totalarea,gap*totalarea),dtype=np.int16)\n",
    "\n",
    "xx=0\n",
    "yy=0\n",
    "\n",
    "dir4 = dir3+'/Max_Projections/'\n",
    "dir50 = dir4+'FOVs/'\n",
    "dir10 = dir4+\"/\"+\"Combined_Files/\"\n",
    "dir11 = dir4+\"/\"+\"Spots/\"\n",
    "files=[]\n",
    "files += [each for each in os.listdir(dir10) if each.endswith('.tif')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "chan = np.char.split(files,sep=\"_\")\n",
    "chan = pd.DataFrame(np.stack(chan,axis=0))[2]\n",
    "chans = pd.unique(chan)\n",
    "\n",
    "genescy7 = ['Dpp10','Dpp8','Dst','Fat1','Itgb4','Larp1','Larp7']\n",
    "genescy5 = ['Apob','Cdh1','Ctnnb1','Cyb5r3','mTor','Net1','Pigr']\n",
    "genescy3 = ['Dync1h1','Dync1i2','Eif4h','Kif1c','Kif3b','Kif5b','None']\n",
    "\n",
    "spotscy7 = pd.DataFrame()\n",
    "spotscy5 = pd.DataFrame()\n",
    "spotscy3 = pd.DataFrame()\n",
    "color = px.colors.qualitative.Light24\n",
    "\n",
    "for fil in range(int(len(files)/4)):\n",
    "\n",
    "#for fil in range(0,2):\n",
    "\n",
    "    imagename = files[len(chans)*fil+3]\n",
    "    print(imagename)\n",
    "    imageloc = dir10+imagename\n",
    "    im2 = tiff.imread(imageloc)\n",
    "    image[xx*gap:xx*gap+im2.shape[1],yy*gap:yy*gap+im2.shape[1]] = im2[0,:,:]\n",
    "\n",
    "    cy7spots = pd.read_csv(dir11+files[len(chans)*fil].split(\".\")[0]+\"_spotclustters.csv\")\n",
    "    cy5spots = pd.read_csv(dir11+files[len(chans)*fil+1].split(\".\")[0]+\"_spotclustters.csv\")\n",
    "    cy3spots = pd.read_csv(dir11+files[len(chans)*fil+2].split(\".\")[0]+\"_spotclustters.csv\")\n",
    "    \n",
    "    cy7spots['Y'] = cy7spots['Y']+xx*gap\n",
    "    cy7spots['X'] = cy7spots['X']+yy*gap\n",
    "    cy5spots['Y'] = cy5spots['Y']+xx*gap\n",
    "    cy5spots['X'] = cy5spots['X']+yy*gap\n",
    "    cy3spots['Y'] = cy3spots['Y']+xx*gap\n",
    "    cy3spots['X'] = cy3spots['X']+yy*gap\n",
    "    \n",
    "    gency7ind = np.array(cy7spots['round'].values)\n",
    "    genecy7 = [genescy7[j] for j in gency7ind]\n",
    "\n",
    "    gency5ind = np.array(cy5spots['round'].values)\n",
    "    genecy5 = [genescy5[j] for j in gency5ind]\n",
    "    \n",
    "    gency3ind = np.array(cy3spots['round'].values)\n",
    "    genecy3 = [genescy3[j] for j in gency3ind]\n",
    "    \n",
    "    cy7spots['gene'] = genecy7\n",
    "    cy5spots['gene'] = genecy5\n",
    "    cy3spots['gene'] = genecy3\n",
    "    \n",
    "    spotscy7 = pd.concat([spotscy7,cy7spots])\n",
    "    spotscy5 = pd.concat([spotscy5,cy5spots])\n",
    "    spotscy3 = pd.concat([spotscy3,cy3spots])\n",
    "    \n",
    "    xx = xx+1\n",
    "    if xx>totalarea-1:\n",
    "        xx=0\n",
    "        yy=yy+1\n",
    "    \n",
    "imagelayer = viewer.add_image(np.uint16(image))\n",
    "imagelayer.contrast_limits=(0,65000)\n",
    "\n",
    "allspots = pd.concat([spotscy7,spotscy5],axis=0)\n",
    "allspots = pd.concat([allspots,spotscy3],axis=0)\n",
    "\n",
    "for round in range(0,len(genescy7)):\n",
    "\n",
    "    cy7 = spotscy7[spotscy7['round']==round]\n",
    "    cy5 = spotscy5[spotscy5['round']==round]\n",
    "    cy3 = spotscy3[spotscy3['round']==round]\n",
    "                \n",
    "    viewer.add_points(np.array(cy7)[:,0:2],\n",
    "                        face_color=color[(len(chans)-1)*round],\n",
    "                        size=5,\n",
    "                        blending='translucent_no_depth',\n",
    "                        edge_width=0,\n",
    "                        name=genescy7[round])\n",
    "\n",
    "    viewer.add_points(np.array(cy5)[:,0:2],\n",
    "                        face_color=color[(len(chans)-1)*round+1],\n",
    "                        size=5,\n",
    "                        blending='translucent_no_depth',\n",
    "                        edge_width=0,\n",
    "                        name=genescy5[round])\n",
    "\n",
    "    viewer.add_points(np.array(cy3)[:,0:2],\n",
    "                        face_color=color[(len(chans)-1)*round+2],\n",
    "                        size=5,\n",
    "                        blending='translucent_no_depth',\n",
    "                        edge_width=0,\n",
    "                        name=genescy3[round])\n",
    "\n",
    "\n",
    "\n",
    "featuresall = allspots['gene']\n",
    "feat = allspots['gene'].values\n",
    "\n",
    "featuresall.to_csv(dir11+'features.csv',index=False)\n",
    "allspots.to_csv(dir11+\"allspots.csv\",index=False)\n",
    "\n",
    "feat = tuple(np.array(featuresall))\n",
    "    \n",
    "pointlayer = viewer.add_points(np.array(allspots)[:,0:2],\n",
    "                                face_color='white',\n",
    "                                size=5,\n",
    "                                blending='translucent_no_depth',\n",
    "                                edge_width=0,\n",
    "                                name='All_spots',\n",
    "                                opacity=0,\n",
    "                                features=feat)    \n",
    "\n",
    "                        \n",
    "pointlayer.refresh()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('multifish')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd4506f47e7a8e319038edbecbacc3135e1dea1a0fd1c74135d4055cf5626a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
