{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os            # Operating system functionality\n",
    "import glob          # File system path manipulation\n",
    "import numpy as np   # Numerical operations\n",
    "import ND2Reader     # Not used in this script\n",
    "import napari        # Interactive multi-dimensional image viewer\n",
    "import bigfish       # Library for single-molecule fluorescence imaging\n",
    "import bigfish.stack as stack        # Functions for processing image stacks\n",
    "import bigfish.detection as detection  # Spot detection algorithms\n",
    "import bigfish.multistack as multistack  # Functions for multi-channel stacks\n",
    "import bigfish.plot as plot          # Plotting utilities\n",
    "import time          # Time tracking\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import tifffile as tiff  # Reading and writing TIFF files\n",
    "from skimage import io   # Image processing functions\n",
    "import plotly.express as px  # Expressive visualization library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = \"./\" ##Input Directory with the ND2 files\n",
    "dir2 = os.path.join(dir1,'TiffFiles') #New subdirectory where processed files will be saved\n",
    "dir3 = os.path.join(dir2, 'Max_Projections')  # Directory for max projections\n",
    "dir4 = os.path.join(dir3, 'FOVs')  # Directory for individual FOVs\n",
    "dir6 = os.path.join(dir3, \"Combined_Files\") # Create a directory to save combined files if it doesn't exist\n",
    "dir7 = os.path.join(dir3, 'Spots') #Save output spot files\n",
    "\n",
    "# Directories to check and create if they don't exist\n",
    "dirs_to_create = [dir2, dir3, dir4, dir6]\n",
    "\n",
    "# Loop through each directory\n",
    "for directory in dirs_to_create:\n",
    "    # Check if the directory does not exist\n",
    "    if not os.path.isdir(directory):\n",
    "        # If it doesn't, create the directory\n",
    "        os.mkdir(directory)\n",
    "\n",
    "# Define parameters\n",
    "voxelval = 110.3752759382\n",
    "##radiusval = 250.0\n",
    "radiusval = 2*voxelval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files as OME Tiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All images are converte to .tif files and saved in the TiffFiles folder\n",
    "- Unclear yet how to change the LUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nis2pyr.convertor import convert_nd2_to_pyramidal_ome_tiff\n",
    "\n",
    "# Filter ND2 files and remove those used for testing bleaching\n",
    "files = [f for f in os.listdir(dir1) if f.endswith('.nd2') and 'Bleach' not in f]\n",
    "\n",
    "# Sort the files\n",
    "files.sort()\n",
    "\n",
    "# Iterate over each file in the 'files' list (all ND2 files)\n",
    "for fil in files:\n",
    "    # Print the directory and filename being processed\n",
    "    print(os.path.join(dir1, fil))\n",
    "    \n",
    "    # Convert the ND2 file to pyramidal OME-TIFF format\n",
    "    # Specify the input ND2 file path, output OME-TIFF file path,\n",
    "    # and the maximum number of pyramid levels (set to 1 in this case)\n",
    "    convert_nd2_to_pyramidal_ome_tiff(os.path.join(dir1, fil), \n",
    "                                    os.path.join(dir2, fil.split(\".\")[0] + '.tif'),\n",
    "                                    max_levels=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Intensity Projection and Image Distribution according to FOV \n",
    "So all images corresponding to the same FOV but across different cycles are saved in one folder \n",
    "- each image for one round is split by its channels and one channel image is saved\n",
    "- still need to figure out how to manage when there are multiple channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all TIFF files in the input directory and sort them\n",
    "files = sorted([f for f in os.listdir(dir2) if f.endswith('.tif')])\n",
    "\n",
    "# Define a function to compute maximum intensity projection along the Z-axis\n",
    "def maximum_intensity_projection(image_stack):\n",
    "    return np.max(image_stack, axis=0)\n",
    "\n",
    "# Loop through each TIFF file\n",
    "for fil in files:\n",
    "    # Read the OME-TIFF file\n",
    "    ome_tiff_path = os.path.join(dir2, fil)\n",
    "    image_stack = tiff.imread(ome_tiff_path)\n",
    "    \n",
    "    # Compute the maximum intensity projection\n",
    "    mip = maximum_intensity_projection(image_stack)\n",
    "    \n",
    "    # Save the max projection image\n",
    "    mip_path = os.path.join(dir3, \"MAX_\" + os.path.splitext(fil)[0] + \".tif\")\n",
    "    tiff.imsave(mip_path, mip)\n",
    "    \n",
    "    # Extract FOV information from the filename\n",
    "    FOV = os.path.splitext(fil)[0].split(\"_\")\n",
    "    \n",
    "    # Create a directory for the FOV if it does not exist\n",
    "    dir5 = os.path.join(dir4, FOV[1])\n",
    "    os.makedirs(dir5, exist_ok=True)\n",
    "    \n",
    "    # Save each channel of the max projection image as a separate TIFF file\n",
    "    for chan, channel_image in enumerate(mip, start=1):\n",
    "        savename = os.path.join(dir5, f\"{FOV[0]}_channel_{chan}.tif\")\n",
    "        tiff.imsave(savename, channel_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Files combining images from single channel and single FOV across all cycles\n",
    "- This helps in easy visualization of the data and can also be used for data analysis? \n",
    "- the files are saved in the Max_Projection/Combined_Files/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of directories in dir3\n",
    "directories = [name for name in os.listdir(dir3) if os.path.isdir(os.path.join(dir3, name))]\n",
    "directories.sort()  # Sort the list of directories\n",
    "\n",
    "# Iterate over each directory\n",
    "for directory in directories:\n",
    "    # Create full path to the current directory\n",
    "    dir_path = os.path.join(dir3, directory)\n",
    "    \n",
    "    # Get list of TIFF files in the directory and sort them\n",
    "    tiff_files = sorted([f for f in os.listdir(dir_path) if f.endswith('.tif')])\n",
    "    \n",
    "    # Extract channel information from TIFF filenames\n",
    "    channels = [f.split('_')[2] for f in tiff_files]\n",
    "    unique_channels = np.unique(channels)\n",
    "    \n",
    "    # Iterate over each unique channel\n",
    "    for channel in unique_channels:\n",
    "        # Get indices of TIFF files corresponding to the current channel\n",
    "        indices = [i for i, c in enumerate(channels) if c == channel]\n",
    "        \n",
    "        # Initialize array for multichannel image\n",
    "        multichannel_image = np.zeros((len(indices), image.shape[0], image.shape[1]))\n",
    "        \n",
    "        # Iterate over each TIFF file corresponding to the current channel\n",
    "        for i, index in enumerate(indices):\n",
    "            # Read TIFF file\n",
    "            img = tiff.imread(os.path.join(dir_path, tiff_files[index]))\n",
    "            # Store image in multichannel array\n",
    "            multichannel_image[i, :, :] = img\n",
    "        \n",
    "        # Define filename for combined multichannel image\n",
    "        savename = os.path.join(dir6, f\"{directory}_channel_{channel}\")\n",
    "        # Save combined multichannel image as TIFF\n",
    "        tiff.imsave(savename, multichannel_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FISH Spot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detecting threshold within multiple FOVs for all rounds and channels\n",
    "- This could potentially be used to get a global threshold value\n",
    "- Alternatively can be used to get individual thresholds for each channel and round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Threshold values for each channel\n",
    "thresholds = [18, 18, 18]  # Thresholds for Channels 1, 2, and 3\n",
    "\n",
    "# Number of cycles for each channel\n",
    "channel_rounds = [8, 8, 8]\n",
    "\n",
    "# Number of FOVs to use to detect thresholds\n",
    "nfovs = 15\n",
    "\n",
    "# Get list of TIFF files in the combined files directory and sort them\n",
    "files = sorted([f for f in os.listdir(dir6) if f.endswith('.tif')])\n",
    "\n",
    "# Compute spot radius in pixels\n",
    "spot_radius_px = detection.get_object_radius_pixel(\n",
    "                    voxel_size_nm=(voxelval, voxelval), \n",
    "                    object_radius_nm=(radiusval, radiusval), \n",
    "                    ndim=2)\n",
    "\n",
    "# Iterate over each channel\n",
    "for channel, threshold, rounds in zip(range(3), thresholds, channel_rounds):\n",
    "    e = 0\n",
    "    for fil in range(nfovs):\n",
    "        print(f\"--- Start {time.time() - start_time} seconds ---\")\n",
    "        filename = files[fil * len(channels) + channel]\n",
    "        img = tiff.imread(os.path.join(dir6, filename))\n",
    "        print(filename)\n",
    "        # Detect spots\n",
    "        for t in range(img.shape[0]):\n",
    "            rna = img[t, :, :]\n",
    "            # LoG filter\n",
    "            rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "            # Local maximum detection\n",
    "            mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "            # Thresholding\n",
    "            threshold_value = detection.automated_threshold_setting(rna_log, mask)\n",
    "            if channel == 0:\n",
    "                ths1[e] = threshold_value\n",
    "            elif channel == 1:\n",
    "                ths2[e] = threshold_value\n",
    "            else:\n",
    "                ths3[e] = threshold_value\n",
    "            e += 1\n",
    "    print(f\"Finished thresholding for channel {channel + 1} after {time.time() - start_time} seconds ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying threshold to detect spots and clusters\n",
    "- spots are saved in the Spots folder as FOV##\\_Channel##\\_#.csv\n",
    "- spot clusters are save in the Spots folder as FOV##\\_Channel##\\_spotclusters_#.csv\n",
    "- clusters information is save in the Spots folder as FOV##\\_Channel##\\_clusters\\_#.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate thresholds for each channel\n",
    "threshs = [2 * np.median(ths) + 40 for ths in [ths1, ths2, ths3]]\n",
    "\n",
    "# Define directory paths and get list of TIFF files\n",
    "files = sorted([each for each in os.listdir(dir6) if each.endswith('.tif')])\n",
    "chan = pd.unique(pd.DataFrame(np.stack(np.char.split(files, sep=\"_\"), axis=0))[2])\n",
    "nfovs = len(files) // len(chan)\n",
    "\n",
    "# Iterate over each FOV\n",
    "for fil in range(nfovs):\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    for cc, chan in enumerate(chans[:-1]):\n",
    "        print(\"Analysing Channel %s\" % cc)\n",
    "        filename = files[fil * len(chans) + cc]\n",
    "        img = tiff.imread(os.path.join(dir6, filename))\n",
    "        base_name = filename.split(\".\")[0]\n",
    "        savenamespots = base_name + \".csv\"\n",
    "        savenamespotscl = base_name + \"_spotclusters.csv\"\n",
    "        savenameclusters = base_name + \"_clusters.csv\"\n",
    "        sp_list, spcl_list, cl_list = [], [], []\n",
    "        # Detect spots and clusters for each round\n",
    "        for t in range(img.shape[0] - 1):\n",
    "            print(\"Analysing Round %s\" % (t + 1))\n",
    "            rna = img[t + 1, :, :]\n",
    "            spots = detection.detect_spots(\n",
    "                images=rna,\n",
    "                return_threshold=False,\n",
    "                threshold=threshs[cc],\n",
    "                voxel_size=(voxelval, voxelval),\n",
    "                spot_radius=(radiusval, radiusval)\n",
    "            )\n",
    "            spots_post_decomposition, dense_regions, reference_spot = detection.decompose_dense(\n",
    "                image=np.uint16(rna),\n",
    "                spots=spots,\n",
    "                voxel_size=(voxelval, voxelval),\n",
    "                spot_radius=(radiusval, radiusval),\n",
    "                alpha=0.75,\n",
    "                beta=0.9,\n",
    "                gamma=15\n",
    "            )\n",
    "            spots_post_clustering, clusters = detection.detect_clusters(\n",
    "                spots=spots_post_decomposition,\n",
    "                voxel_size=(int(voxelval), int(voxelval)),\n",
    "                radius=int(radiusval),\n",
    "                nb_min_spots=4\n",
    "            )\n",
    "            sp_list.append(spots)\n",
    "            spcl_list.append(spots_post_clustering)\n",
    "            cl_list.append(clusters)\n",
    "        sp = pd.DataFrame(np.vstack(sp_list), columns=['Y', 'X', 'round'])\n",
    "        spcl = pd.DataFrame(np.vstack(spcl_list), columns=['Y', 'X', 'clusterindex', 'round'])\n",
    "        cl = pd.DataFrame(np.vstack(cl_list), columns=['Y', 'X', 'nspots', 'index', 'round'])\n",
    "        sp.to_csv(os.path.join(dir6, savenamespots), index=False)\n",
    "        spcl.to_csv(os.path.join(dir6, savenamespotscl), index=False)\n",
    "        cl.to_csv(os.path.join(dir6, savenameclusters), index=False)\n",
    "\n",
    "print(\"Finished thresholding for channel 1 image %s after %s seconds ---\" % (e, time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Napari viewer\n",
    "- View all images in one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the nearest square number below a given limit\n",
    "def nearest_square(limit):\n",
    "    return int(limit ** 0.5)\n",
    "\n",
    "# Get list of TIFF files\n",
    "files = sorted([each for each in os.listdir(dir6) if each.endswith('.tif')])\n",
    "chan = pd.unique(pd.DataFrame(np.stack(np.char.split(files, sep=\"_\"), axis=0))[2])\n",
    "nfovs = int(len(files) / len(chan))\n",
    "\n",
    "# Initialize empty lists to store spot data for each channel\n",
    "spotscy7, spotscy5, spotscy3 = [], [], []\n",
    "\n",
    "# Read all spot files for each channel before entering the loop\n",
    "for fil in range(nfovs):\n",
    "    for cc in range(len(chans)):\n",
    "        filename = files[fil * len(chans) + cc]\n",
    "        cy_spots = pd.read_csv(os.path.join(dir7, filename.split(\".\")[0] + \"_spotclustters.csv\"))\n",
    "        cy_spots['Y'] += fil * gap\n",
    "        cy_spots['X'] += fil * gap\n",
    "        if cc == 0:\n",
    "            spotscy7.append(cy_spots)\n",
    "        elif cc == 1:\n",
    "            spotscy5.append(cy_spots)\n",
    "        elif cc == 2:\n",
    "            spotscy3.append(cy_spots)\n",
    "\n",
    "# Concatenate spot data for each channel\n",
    "spotscy7 = pd.concat(spotscy7)\n",
    "spotscy5 = pd.concat(spotscy5)\n",
    "spotscy3 = pd.concat(spotscy3)\n",
    "\n",
    "# Initialize Napari viewer\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# Add image to Napari viewer\n",
    "imagelayer = viewer.add_image(np.zeros((gap * totalarea, gap * totalarea), dtype=np.int16))\n",
    "imagelayer.contrast_limits = (0, 65000)\n",
    "\n",
    "# Iterate over each gene and add points to the viewer for each channel\n",
    "for round in range(len(genescy7)):\n",
    "    cy7 = spotscy7[spotscy7['round'] == round]\n",
    "    cy5 = spotscy5[spotscy5['round'] == round]\n",
    "    cy3 = spotscy3[spotscy3['round'] == round]\n",
    "\n",
    "    viewer.add_points(np.array(cy7)[:, :2], face_color=color[(len(chans) - 1) * round], size=5,\n",
    "                      blending='translucent_no_depth', edge_width=0, name=genescy7[round])\n",
    "\n",
    "    viewer.add_points(np.array(cy5)[:, :2], face_color=color[(len(chans) - 1) * round + 1], size=5,\n",
    "                      blending='translucent_no_depth', edge_width=0, name=genescy5[round])\n",
    "\n",
    "    viewer.add_points(np.array(cy3)[:, :2], face_color=color[(len(chans) - 1) * round + 2], size=5,\n",
    "                      blending='translucent_no_depth', edge_width=0, name=genescy3[round])\n",
    "\n",
    "# Concatenate spot data for all channels\n",
    "allspots = pd.concat([spotscy7, spotscy5, spotscy3])\n",
    "\n",
    "# Save spot data to CSV files\n",
    "allspots.to_csv(os.path.join(dir7, \"allspots.csv\"), index=False)\n",
    "featuresall = allspots['gene']\n",
    "featuresall.to_csv(os.path.join(dir7, 'features.csv'), index=False)\n",
    "\n",
    "# Add points for all spots to the viewer\n",
    "feat = tuple(np.array(featuresall))\n",
    "pointlayer = viewer.add_points(np.array(allspots)[:, :2], face_color='white', size=5,\n",
    "                                blending='translucent_no_depth', edge_width=0, name='All_spots', opacity=0,\n",
    "                                features=feat)\n",
    "\n",
    "pointlayer.refresh()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('multifish')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd4506f47e7a8e319038edbecbacc3135e1dea1a0fd1c74135d4055cf5626a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
