{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os            # Operating system functionality\n",
    "import glob          # File system path manipulation\n",
    "import numpy as np   # Numerical operations\n",
    "#import nd2reader     # Not used in this script\n",
    "import napari        # Interactive multi-dimensional image viewer\n",
    "import bigfish       # Library for single-molecule fluorescence imaging\n",
    "import bigfish.stack as stack        # Functions for processing image stacks\n",
    "import bigfish.detection as detection  # Spot detection algorithms\n",
    "import bigfish.multistack as multistack  # Functions for multi-channel stacks\n",
    "import bigfish.plot as plot          # Plotting utilities\n",
    "import time          # Time tracking\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import tifffile as tiff  # Reading and writing TIFF files\n",
    "from skimage import io   # Image processing functions\n",
    "import plotly.express as px  # Expressive visualization library\n",
    "from nis2pyr.convertor import convert_nd2_to_pyramidal_ome_tiff\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_ROOT = \"./Test/\" ##Input Directory with the ND2 files\n",
    "DIR_TIFFFILES = os.path.join(DIR_ROOT,'TiffFiles') #New subdirectory where processed files will be saved\n",
    "DIR_MAXPROJ = os.path.join(DIR_TIFFFILES, 'Max_Projections')  # Directory for max projections\n",
    "DIR_FOVS = os.path.join(DIR_MAXPROJ, 'FOVs')  # Directory for individual FOVs\n",
    "DIR_COMBINED_FILES = os.path.join(DIR_MAXPROJ, \"Combined_Files\") # a directory to save combined files if it doesn't exist\n",
    "DIR_SPOTS = os.path.join(DIR_MAXPROJ, 'Spots') #Save output spot files\n",
    "\n",
    "# Directories to check and create if they don't exist\n",
    "dirs_to_create = [DIR_TIFFFILES, DIR_MAXPROJ, DIR_FOVS, DIR_COMBINED_FILES, DIR_SPOTS]\n",
    "\n",
    "# Loop through each directory\n",
    "for directory in dirs_to_create:\n",
    "    # Check if the directory does not exist\n",
    "    if not os.path.isdir(directory):\n",
    "        # If it doesn't, create the directory\n",
    "        os.mkdir(directory)\n",
    "\n",
    "# Define parameters\n",
    "voxelval = 110.3752759382\n",
    "##radiusval = 250.0\n",
    "radiusval = 2*voxelval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files as OME Tiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All images are converte to .tif files and saved in the TiffFiles folder\n",
    "- Unclear yet how to change the LUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Test/Round00_FOV0000.nd2\n",
      "Reading ./Test/Round00_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round00_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round00_FOV0001.nd2\n",
      "Reading ./Test/Round00_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round00_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round01_FOV0000.nd2\n",
      "Reading ./Test/Round01_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round01_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round01_FOV0001.nd2\n",
      "Reading ./Test/Round01_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round01_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round02_FOV0000.nd2\n",
      "Reading ./Test/Round02_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round02_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round02_FOV0001.nd2\n",
      "Reading ./Test/Round02_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round02_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round03_FOV0000.nd2\n",
      "Reading ./Test/Round03_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round03_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round03_FOV0001.nd2\n",
      "Reading ./Test/Round03_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round03_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round04_FOV0000.nd2\n",
      "Reading ./Test/Round04_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round04_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round04_FOV0001.nd2\n",
      "Reading ./Test/Round04_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round04_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round05_FOV0000.nd2\n",
      "Reading ./Test/Round05_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round05_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round05_FOV0001.nd2\n",
      "Reading ./Test/Round05_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round05_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round06_FOV0000.nd2\n",
      "Reading ./Test/Round06_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round06_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round06_FOV0001.nd2\n",
      "Reading ./Test/Round06_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round06_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round07_FOV0000.nd2\n",
      "Reading ./Test/Round07_FOV0000.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round07_FOV0000.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n",
      "./Test/Round07_FOV0001.nd2\n",
      "Reading ./Test/Round07_FOV0001.nd2\n",
      "ND2 dimensions: {'Z': 15, 'C': 4, 'Y': 2720, 'X': 2720}; RGB: False; datatype: uint16; legacy: False\n",
      "Saving pyramidal OME TIFF file ./Test/TiffFiles/Round07_FOV0001.tif\n",
      "Writing level 0: TZCYXS=(1, 15, 4, 2720, 2720, 1)\n",
      "Updating OME XML channel names and colors\n"
     ]
    }
   ],
   "source": [
    "# Filter ND2 files and remove those used for testing bleaching\n",
    "files = sorted([f for f in os.listdir(DIR_ROOT) if f.endswith('.nd2') and 'Bleach' not in f])\n",
    "\n",
    "# Iterate over each file in the 'files' list (all ND2 files)\n",
    "for fil in files:\n",
    "    # Print the directory and filename being processed\n",
    "    print(os.path.join(DIR_ROOT, fil))\n",
    "    \n",
    "    # Convert the ND2 file to pyramidal OME-TIFF format\n",
    "    # Specify the input ND2 file path, output OME-TIFF file path,\n",
    "    # and the maximum number of pyramid levels (set to 1 in this case)\n",
    "    convert_nd2_to_pyramidal_ome_tiff(os.path.join(DIR_ROOT, fil), \n",
    "                                    os.path.join(DIR_TIFFFILES, fil.split(\".\")[0] + '.tif'),\n",
    "                                    max_levels=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Intensity Projection and Image Distribution according to FOV \n",
    "So all images corresponding to the same FOV but across different cycles are saved in one folder \n",
    "- each image for one round is split by its channels and one channel image is saved\n",
    "- still need to figure out how to manage when there are multiple channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all TIFF files in the input directory and sort them\n",
    "files = sorted([f for f in os.listdir(DIR_TIFFFILES) if f.endswith('.tif')])\n",
    "\n",
    "# Define a function to compute maximum intensity projection along the Z-axis\n",
    "def maximum_intensity_projection(image_stack):\n",
    "    return np.max(image_stack, axis=0)\n",
    "\n",
    "# Loop through each TIFF file\n",
    "for fil in files:\n",
    "    # Read the OME-TIFF file\n",
    "    ome_tiff_path = os.path.join(DIR_TIFFFILES, fil)\n",
    "    image_stack = tiff.imread(ome_tiff_path)\n",
    "    \n",
    "    # Compute the maximum intensity projection\n",
    "    mip = maximum_intensity_projection(image_stack)\n",
    "    \n",
    "    # Save the max projection image\n",
    "    mip_path = os.path.join(DIR_MAXPROJ, \"MAX_\" + os.path.splitext(fil)[0] + \".tif\")\n",
    "    tiff.imsave(mip_path, mip)\n",
    "    \n",
    "    # Extract FOV information from the filename\n",
    "    FOV = os.path.splitext(fil)[0].split(\"_\")\n",
    "    \n",
    "    # Create a directory for the FOV if it does not exist\n",
    "    dir5 = os.path.join(DIR_FOVS, FOV[1])\n",
    "    os.makedirs(dir5, exist_ok=True)\n",
    "    \n",
    "    # Save each channel of the max projection image as a separate TIFF file\n",
    "    for chan, channel_image in enumerate(mip, start=1):\n",
    "        savename = os.path.join(dir5, f\"{FOV[0]}_channel_{chan}.tif\")\n",
    "        tiff.imsave(savename, channel_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Files combining images from single channel and single FOV across all cycles\n",
    "- This helps in easy visualization of the data and can also be used for data analysis? \n",
    "- the files are saved in the Max_Projection/Combined_Files/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of directories in DIR_MAXPROJ\n",
    "directories = [name for name in os.listdir(DIR_FOVS) if os.path.isdir(os.path.join(DIR_FOVS, name))]\n",
    "directories.sort()  # Sort the list of directories\n",
    "\n",
    "# Iterate over each directory\n",
    "for directory in directories:\n",
    "    # Create full path to the current directory\n",
    "    dir_path = os.path.join(DIR_FOVS, directory)\n",
    "    \n",
    "    # Get list of TIFF files in the directory and sort them\n",
    "    tiff_files = sorted([f for f in os.listdir(dir_path) if f.endswith('.tif')])\n",
    "    \n",
    "    # Extract channel information from TIFF filenames\n",
    "    channels = [f.split('_')[2] for f in tiff_files]\n",
    "    unique_channels = np.unique(channels)\n",
    "    \n",
    "    # Iterate over each unique channel\n",
    "    for channel in unique_channels:\n",
    "        # Get indices of TIFF files corresponding to the current channel\n",
    "        indices = [i for i, c in enumerate(channels) if c == channel]\n",
    "        \n",
    "        # Initialize array for multichannel image\n",
    "        multichannel_image = np.zeros((len(indices), channel_image.shape[0], channel_image.shape[1]))\n",
    "        \n",
    "        # Iterate over each TIFF file corresponding to the current channel\n",
    "        for i, index in enumerate(indices):\n",
    "            # Read TIFF file\n",
    "            img = tiff.imread(os.path.join(dir_path, tiff_files[index]))\n",
    "            # Store image in multichannel array\n",
    "            multichannel_image[i, :, :] = img\n",
    "        \n",
    "        # Define filename for combined multichannel image\n",
    "        savename = os.path.join(DIR_COMBINED_FILES, f\"{directory}_channel_{channel}\")\n",
    "        # Save combined multichannel image as TIFF\n",
    "        tiff.imsave(savename, multichannel_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FISH Spot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detecting threshold within multiple FOVs for all rounds and channels\n",
    "- This could potentially be used to get a global threshold value\n",
    "- Alternatively can be used to get individual thresholds for each channel and round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start 0.011390924453735352 seconds ---\n",
      "0\n",
      "0\n",
      "4\n",
      "FOV0000_channel_1.tif\n",
      "--- Start 6.827385902404785 seconds ---\n",
      "1\n",
      "0\n",
      "4\n",
      "FOV0001_channel_1.tif\n",
      "Finished thresholding for channel 1 after 17.019725799560547 seconds ---\n",
      "--- Start 17.01987886428833 seconds ---\n",
      "0\n",
      "1\n",
      "4\n",
      "FOV0000_channel_2.tif\n",
      "--- Start 28.690464973449707 seconds ---\n",
      "1\n",
      "1\n",
      "4\n",
      "FOV0001_channel_2.tif\n",
      "Finished thresholding for channel 2 after 40.99107480049133 seconds ---\n",
      "--- Start 40.991265058517456 seconds ---\n",
      "0\n",
      "2\n",
      "4\n",
      "FOV0000_channel_3.tif\n",
      "--- Start 52.88156795501709 seconds ---\n",
      "1\n",
      "2\n",
      "4\n",
      "FOV0001_channel_3.tif\n",
      "Finished thresholding for channel 3 after 64.80895280838013 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Threshold values for each channel\n",
    "thresholds = [18, 18, 18]  # Thresholds for Channels 1, 2, and 3\n",
    "\n",
    "# Number of cycles for each channel\n",
    "channel_rounds = [8, 8, 8]\n",
    "\n",
    "# Number of FOVs to use to detect thresholds\n",
    "nfovs = 2\n",
    "\n",
    "# Get list of TIFF files in the combined files directory and sort them\n",
    "files = sorted([f for f in os.listdir(DIR_COMBINED_FILES) if f.endswith('.tif')])\n",
    "\n",
    "# Compute spot radius in pixels\n",
    "spot_radius_px = detection.get_object_radius_pixel(\n",
    "                    voxel_size_nm=(voxelval, voxelval), \n",
    "                    object_radius_nm=(radiusval, radiusval), \n",
    "                    ndim=2)\n",
    "\n",
    "ths1 = np.zeros((channel_rounds[0]*nfovs, 1))\n",
    "ths2 = np.zeros((channel_rounds[1]*nfovs, 1))\n",
    "ths3 = np.zeros((channel_rounds[2]*nfovs, 1))\n",
    "\n",
    "# Iterate over each channel\n",
    "for channel, threshold, rounds in zip(range(3), thresholds, channel_rounds):\n",
    "    e = 0\n",
    "    for fil in range(nfovs):\n",
    "        print(f\"--- Start {time.time() - start_time} seconds ---\")\n",
    "        print(fil)\n",
    "        print(channel)\n",
    "        print(len(unique_channels))\n",
    "        \n",
    "        filename = files[fil * len(unique_channels) + channel]\n",
    "        img = tiff.imread(os.path.join(DIR_COMBINED_FILES, filename))\n",
    "        print(filename)\n",
    "        # Detect spots\n",
    "        for t in range(img.shape[0]):\n",
    "            rna = img[t, :, :]\n",
    "            # LoG filter\n",
    "            rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "            # Local maximum detection\n",
    "            mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "            # Thresholding\n",
    "            threshold_value = detection.automated_threshold_setting(rna_log, mask)\n",
    "            if channel == 0:\n",
    "                ths1[e] = threshold_value\n",
    "            elif channel == 1:\n",
    "                ths2[e] = threshold_value\n",
    "            else:\n",
    "                ths3[e] = threshold_value\n",
    "            e += 1\n",
    "    print(f\"Finished thresholding for channel {channel + 1} after {time.time() - start_time} seconds ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying threshold to detect spots and clusters\n",
    "- spots are saved in the Spots folder as FOV##\\_Channel##\\_#.csv\n",
    "- spot clusters are save in the Spots folder as FOV##\\_Channel##\\_spotclusters_#.csv\n",
    "- clusters information is save in the Spots folder as FOV##\\_Channel##\\_clusters\\_#.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start 0.011821985244750977 seconds ---\n",
      "0\n",
      "Analysing Channel 0\n",
      "FOV0000_channel_1.tif\n",
      "Analysing Round 1\n",
      "Analysing Round 2\n",
      "Analysing Round 3\n",
      "Analysing Round 4\n",
      "Analysing Round 5\n",
      "Analysing Round 6\n",
      "Analysing Round 7\n",
      "Analysing Channel 1\n",
      "FOV0000_channel_2.tif\n",
      "Analysing Round 1\n",
      "Analysing Round 2\n",
      "Analysing Round 3\n",
      "Analysing Round 4\n",
      "Analysing Round 5\n",
      "Analysing Round 6\n",
      "Analysing Round 7\n",
      "Analysing Channel 2\n",
      "FOV0000_channel_3.tif\n",
      "Analysing Round 1\n",
      "Analysing Round 2\n",
      "Analysing Round 3\n",
      "Analysing Round 4\n",
      "Analysing Round 5\n",
      "Analysing Round 6\n",
      "Analysing Round 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asrivath/opt/anaconda3/envs/fish/lib/python3.7/site-packages/bigfish/detection/utils.py:433: UserWarning: Problem occurs during the computation of a reference spot. Not enough (uncropped) spots have been detected.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Start 95.07934212684631 seconds ---\n",
      "1\n",
      "Analysing Channel 0\n",
      "FOV0001_channel_1.tif\n",
      "Analysing Round 1\n",
      "Analysing Round 2\n",
      "Analysing Round 3\n",
      "Analysing Round 4\n",
      "Analysing Round 5\n",
      "Analysing Round 6\n",
      "Analysing Round 7\n",
      "Analysing Channel 1\n",
      "FOV0001_channel_2.tif\n",
      "Analysing Round 1\n",
      "Analysing Round 2\n",
      "Analysing Round 3\n",
      "Analysing Round 4\n",
      "Analysing Round 5\n",
      "Analysing Round 6\n",
      "Analysing Round 7\n",
      "Analysing Channel 2\n",
      "FOV0001_channel_3.tif\n",
      "Analysing Round 1\n",
      "Analysing Round 2\n",
      "Analysing Round 3\n",
      "Analysing Round 4\n",
      "Analysing Round 5\n",
      "Analysing Round 6\n",
      "Analysing Round 7\n",
      "Finished thresholding for channel 1 image 16 after 196.73482179641724 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asrivath/opt/anaconda3/envs/fish/lib/python3.7/site-packages/bigfish/detection/utils.py:433: UserWarning: Problem occurs during the computation of a reference spot. Not enough (uncropped) spots have been detected.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Compute thresholds\n",
    "thresholds = [2 * np.median(ths) + 40 for ths in [ths1, ths2, ths3]]\n",
    "\n",
    "# Get list of TIFF files\n",
    "files = sorted([each for each in os.listdir(DIR_COMBINED_FILES) if each.endswith('.tif')])\n",
    "nfovs = len(files) // len(unique_channels)\n",
    "\n",
    "\n",
    "for fil in range(nfovs):\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    print(fil)\n",
    "    for cc, channel in enumerate(unique_channels[0:-1]):\n",
    "        print(\"Analysing Channel %s\" % cc)\n",
    "        filename = files[fil * len(unique_channels) + cc]\n",
    "        print(filename)\n",
    "        img = tiff.imread(os.path.join(DIR_COMBINED_FILES, filename))\n",
    "        savedir = os.path.join(DIR_FOVS, \"Combined_Files\")\n",
    "        savenamespots = filename.split(\".\")[0] + \".csv\"\n",
    "        savenamespotscl = filename.split(\".\")[0] + \"_spotclusters.csv\"\n",
    "        savenameclusters = filename.split(\".\")[0] + \"_clusters.csv\"\n",
    "\n",
    "        sp = pd.DataFrame()\n",
    "        spcl = pd.DataFrame()\n",
    "        cl = pd.DataFrame()\n",
    "\n",
    "        # Detect spots\n",
    "        for t in range(img.shape[0] - 1):\n",
    "            print(\"Analysing Round %s\" % (t + 1))\n",
    "            rna = img[t + 1, :, :]\n",
    "            # LoG filter\n",
    "            spots = detection.detect_spots(\n",
    "                images=rna,\n",
    "                return_threshold=False,\n",
    "                threshold=thresholds[cc],\n",
    "                voxel_size=(voxelval, voxelval),  # in nanometer (one value per dimension zyx)\n",
    "                spot_radius=(radiusval, radiusval)  # in nanometer (one value per dimension zyx)\n",
    "            )\n",
    "\n",
    "            spots_post_decomposition, dense_regions, reference_spot = detection.decompose_dense(\n",
    "                image=np.uint16(rna),\n",
    "                spots=spots,\n",
    "                voxel_size=(voxelval, voxelval),\n",
    "                spot_radius=(radiusval, radiusval),\n",
    "                alpha=0.75,  # alpha impacts the number of spots per candidate region\n",
    "                beta=0.9,  # beta impacts the number of candidate regions to decompose\n",
    "                gamma=15  # gamma the filtering step to denoise the image\n",
    "            )\n",
    "\n",
    "            spots_post_clustering, clusters = detection.detect_clusters(\n",
    "                spots=spots_post_decomposition,\n",
    "                voxel_size=(int(voxelval), int(voxelval)),\n",
    "                radius=int(radiusval),\n",
    "                nb_min_spots=4\n",
    "            )\n",
    "\n",
    "            spotspd = pd.DataFrame(spots)\n",
    "            spclpd = pd.DataFrame(spots_post_clustering)\n",
    "            clupd = pd.DataFrame(clusters)\n",
    "\n",
    "            spotspd['round'] = t\n",
    "            spclpd['round'] = t\n",
    "            clupd['round'] = t\n",
    "\n",
    "            sp = pd.concat([sp, spotspd])\n",
    "            spcl = pd.concat([spcl, spclpd])\n",
    "            cl = pd.concat([cl, clupd])\n",
    "\n",
    "        sp.columns = ['Y', 'X', 'round']\n",
    "        cl.columns = ['Y', 'X', 'nspots', 'index', 'round']\n",
    "        spcl.columns = ['Y', 'X', 'clusterindex', 'round']\n",
    "\n",
    "        sp.to_csv(os.path.join(DIR_SPOTS, savenamespots), index=False)\n",
    "        spcl.to_csv(os.path.join(DIR_SPOTS, savenamespotscl), index=False)\n",
    "        cl.to_csv(os.path.join(DIR_SPOTS, savenameclusters), index=False)\n",
    "\n",
    "print(\"Finished thresholding for channel 1 image %s after %s seconds ---\" % (e, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Napari viewer\n",
    "- View all images in one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOV0000_channel_4.tif\n",
      "FOV0001_channel_4.tif\n",
      "0\n",
      "4\n",
      "8\n",
      "12\n",
      "16\n",
      "20\n",
      "24\n",
      "21\n",
      "25\n",
      "29\n",
      "33\n",
      "37\n",
      "41\n",
      "45\n",
      "42\n",
      "46\n",
      "50\n",
      "54\n",
      "58\n",
      "62\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "def nearest_square(limit):\n",
    "    return int(math.ceil(limit ** 0.5))\n",
    "\n",
    "# Initialize Napari viewer\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# Define parameters\n",
    "\n",
    "GAP_SIZE = 2800\n",
    "DIR_MAXPROJ = DIR_TIFFFILES + '/Max_Projections/'\n",
    "DIR_COMBINED_FILES = DIR_MAXPROJ + 'Combined_Files/'\n",
    "DIR_SPOTS = DIR_MAXPROJ + 'Spots/'\n",
    "files = sorted([f for f in os.listdir(DIR_COMBINED_FILES) if f.endswith('.tif')])\n",
    "channels = pd.unique(pd.DataFrame(np.stack(np.char.split(files, sep=\"_\"), axis=0))[2])\n",
    "nfovs = len(files) // len(channels)\n",
    "\n",
    "genes_cy7 = ['Dpp10', 'Dpp8', 'Dst', 'Fat1', 'Itgb4', 'Larp1', 'Larp7']\n",
    "genes_cy5 = ['Apob', 'Cdh1', 'Ctnnb1', 'Cyb5r3', 'mTor', 'Net1', 'Pigr']\n",
    "genes_cy3 = ['Dync1h1', 'Dync1i2', 'Eif4h', 'Kif1c', 'Kif3b', 'Kif5b', 'None']\n",
    "color = px.colors.qualitative.Light24\n",
    "\n",
    "# Initialize variables\n",
    "spots_cy7 = pd.DataFrame()\n",
    "spots_cy5 = pd.DataFrame()\n",
    "spots_cy3 = pd.DataFrame()\n",
    "total_area = nearest_square(len(files) / 4)\n",
    "image = np.zeros((GAP_SIZE * total_area, GAP_SIZE * total_area), dtype=np.int16)\n",
    "xx = 0\n",
    "yy = 0\n",
    "\n",
    "for fov in range(nfovs):\n",
    "    image_name = files[len(channels) * fov + 3]\n",
    "    print(image_name)\n",
    "    image_loc = os.path.join(DIR_COMBINED_FILES, image_name)\n",
    "    im2 = tiff.imread(image_loc)\n",
    "    image[xx * GAP_SIZE:xx * GAP_SIZE + im2.shape[1], yy * GAP_SIZE:yy * GAP_SIZE + im2.shape[1]] = im2[0, :, :]\n",
    "\n",
    "    for cc, genes in enumerate([genes_cy7, genes_cy5, genes_cy3]):\n",
    "        file_name = files[fov * len(channels) + cc]\n",
    "        spots = pd.read_csv(os.path.join(DIR_SPOTS, file_name.split(\".\")[0] + \"_spotclusters.csv\"))\n",
    "        spots['Y'] += xx * GAP_SIZE\n",
    "        spots['X'] += yy * GAP_SIZE\n",
    "        spots['gene'] = spots['round'].apply(lambda x: genes[x]).values\n",
    "        if cc == 0:\n",
    "            spots_cy7 = pd.concat([spots_cy7, spots])\n",
    "        elif cc == 1:\n",
    "            spots_cy5 = pd.concat([spots_cy5, spots])\n",
    "        elif cc == 2:\n",
    "            spots_cy3 = pd.concat([spots_cy3, spots])\n",
    "\n",
    "    xx += 1\n",
    "    if xx > total_area - 1:\n",
    "        xx = 0\n",
    "        yy += 1\n",
    "\n",
    "# Add image to viewer\n",
    "image_layer = viewer.add_image(np.uint16(image))\n",
    "image_layer.contrast_limits = (0, 65000)\n",
    "\n",
    "# Concatenate all spots\n",
    "all_spots = pd.concat([spots_cy7, spots_cy5, spots_cy3])\n",
    "count=0\n",
    "# Add spots to viewer\n",
    "for round, genes in enumerate([genes_cy7, genes_cy5, genes_cy3]):\n",
    "    \n",
    "    for cc, round in enumerate(genes):\n",
    "        cy = all_spots[all_spots['gene'] == round]\n",
    "        viewer.add_points(np.array(cy)[:, :2], face_color=color[count],\n",
    "                          size=5, blending='translucent_no_depth', edge_width=0, name=round)\n",
    "        print(count)\n",
    "        count = count+1\n",
    "        if count>len(color):\n",
    "            count=0\n",
    "            \n",
    "# Export features and all spots\n",
    "all_spots['gene'].to_csv(os.path.join(DIR_SPOTS, 'features.csv'), index=False)\n",
    "all_spots.to_csv(os.path.join(DIR_SPOTS, \"allspots.csv\"), index=False)\n",
    "\n",
    "feat = tuple(np.array(all_spots['gene']))\n",
    "point_layer = viewer.add_points(np.array(all_spots)[:, :2], face_color='white', size=5,\n",
    "                                 blending='translucent_no_depth', edge_width=0, name='All_spots', opacity=0,\n",
    "                                 features=feat)\n",
    "\n",
    "point_layer.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[714 2308]\n",
      " [727 2310]\n",
      " [763 2319]\n",
      " [777 2262]\n",
      " [806 2353]\n",
      " [827 2317]\n",
      " [869 2289]\n",
      " [1083 900]\n",
      " [1305 1237]\n",
      " [1309 2207]\n",
      " [1326 2162]\n",
      " [1369 1695]\n",
      " [1597 963]\n",
      " [1721 1537]\n",
      " [1869 1189]\n",
      " [1893 316]\n",
      " [2006 1193]\n",
      " [2037 1394]\n",
      " [2205 1028]\n",
      " [2229 990]\n",
      " [2414 2218]\n",
      " [2437 2249]\n",
      " [2499 1458]\n",
      " [2519 1508]\n",
      " [2539 1498]\n",
      " [2548 1491]\n",
      " [2573 679]\n",
      " [277 2101]\n",
      " [638 2288]\n",
      " [637 2288]\n",
      " [634 2289]\n",
      " [693 2304]\n",
      " [709 2269]\n",
      " [733 2194]\n",
      " [754 2325]\n",
      " [754 2325]\n",
      " [773 2316]\n",
      " [773 2325]\n",
      " [782 2178]\n",
      " [813 2191]\n",
      " [826 2253]\n",
      " [843 2173]\n",
      " [937 2291]\n",
      " [937 2291]\n",
      " [937 2291]\n",
      " [939 2288]\n",
      " [1154 1902]\n",
      " [1331 43]\n",
      " [1469 866]\n",
      " [1516 835]\n",
      " [1810 1899]\n",
      " [1813 1211]\n",
      " [1818 371]\n",
      " [1817 368]\n",
      " [1821 370]\n",
      " [1883 1118]\n",
      " [1884 1118]\n",
      " [2053 2003]\n",
      " [2091 1364]\n",
      " [2287 2314]\n",
      " [2326 2302]\n",
      " [2325 2303]\n",
      " [2331 2321]\n",
      " [2344 2304]\n",
      " [2338 2310]\n",
      " [2342 2306]\n",
      " [2335 2307]\n",
      " [2450 2239]\n",
      " [2449 2240]\n",
      " [2488 1002]\n",
      " [2489 1001]\n",
      " [2492 1001]\n",
      " [2488 1002]\n",
      " [2665 2157]\n",
      " [2666 2154]\n",
      " [2664 2160]\n",
      " [3119 837]\n",
      " [3152 839]\n",
      " [3154 671]\n",
      " [3219 844]\n",
      " [3225 646]\n",
      " [3245 680]\n",
      " [3248 807]\n",
      " [3270 729]\n",
      " [3788 1509]\n",
      " [3797 1670]\n",
      " [3974 1141]\n",
      " [4107 1690]\n",
      " [4186 775]\n",
      " [4189 815]\n",
      " [4204 2051]\n",
      " [4228 2050]\n",
      " [4236 799]\n",
      " [4259 1989]\n",
      " [4306 866]\n",
      " [4313 882]\n",
      " [4395 2438]\n",
      " [4479 1070]\n",
      " [4530 1184]\n",
      " [4532 1279]\n",
      " [4556 1532]\n",
      " [4560 1497]\n",
      " [4573 2100]\n",
      " [4575 2231]\n",
      " [4594 2023]\n",
      " [4687 601]\n",
      " [4728 2027]\n",
      " [4729 2044]\n",
      " [4810 1299]\n",
      " [4817 258]\n",
      " [4926 1211]\n",
      " [4929 1292]\n",
      " [5158 1152]\n",
      " [5201 2217]\n",
      " [5348 1054]\n",
      " [3044 736]\n",
      " [3106 654]\n",
      " [3172 693]\n",
      " [3242 636]\n",
      " [3245 636]\n",
      " [3242 633]\n",
      " [3254 637]\n",
      " [3242 637]\n",
      " [3246 638]\n",
      " [3245 631]\n",
      " [3239 634]\n",
      " [3248 637]\n",
      " [3253 640]\n",
      " [3232 638]\n",
      " [3236 637]\n",
      " [3240 630]\n",
      " [3247 633]\n",
      " [3239 666]\n",
      " [3239 661]\n",
      " [3250 669]\n",
      " [3262 715]\n",
      " [3293 726]\n",
      " [3297 726]\n",
      " [3290 725]\n",
      " [3727 1253]\n",
      " [3731 1255]\n",
      " [3731 1255]\n",
      " [3731 1255]\n",
      " [3908 1650]\n",
      " [4053 1669]\n",
      " [4043 1677]\n",
      " [4050 1672]\n",
      " [4040 1674]\n",
      " [4050 1667]\n",
      " [4056 1664]\n",
      " [4046 1675]\n",
      " [4060 1668]\n",
      " [4060 1674]\n",
      " [4046 1669]\n",
      " [4053 1662]\n",
      " [4043 1671]\n",
      " [4055 1667]\n",
      " [4061 1663]\n",
      " [4054 1674]\n",
      " [4047 1677]\n",
      " [4047 1663]\n",
      " [4134 1640]\n",
      " [4137 1638]\n",
      " [4141 1640]\n",
      " [4137 1645]\n",
      " [4132 1635]\n",
      " [4131 1646]\n",
      " [4141 1636]\n",
      " [4137 1633]\n",
      " [4131 1639]\n",
      " [4197 1726]\n",
      " [4251 816]\n",
      " [4273 875]\n",
      " [4282 877]\n",
      " [4283 877]\n",
      " [4278 875]\n",
      " [4282 872]\n",
      " [4293 801]\n",
      " [4295 858]\n",
      " [4299 837]\n",
      " [4373 2105]\n",
      " [4377 2100]\n",
      " [4455 2176]\n",
      " [4559 2235]\n",
      " [4563 1962]\n",
      " [4596 1251]\n",
      " [4634 1111]\n",
      " [4672 1965]\n",
      " [4701 1577]\n",
      " [4698 1575]\n",
      " [4724 1146]\n",
      " [4726 1124]\n",
      " [4726 1124]\n",
      " [4733 1173]\n",
      " [4741 2655]\n",
      " [4742 2659]\n",
      " [4941 1081]\n",
      " [4941 1082]\n",
      " [4937 1080]\n",
      " [4945 1064]\n",
      " [4945 1064]\n",
      " [4944 1063]\n",
      " [4982 1141]\n",
      " [4983 1140]\n",
      " [4978 1142]\n",
      " [5291 1081]\n",
      " [5345 1043]\n",
      " [5396 2335]\n",
      " [5516 1984]]\n",
      "Dpp10\n"
     ]
    }
   ],
   "source": [
    "print(np.array(cy)[:, :2])\n",
    "print(round)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('multifish')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd4506f47e7a8e319038edbecbacc3135e1dea1a0fd1c74135d4055cf5626a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
