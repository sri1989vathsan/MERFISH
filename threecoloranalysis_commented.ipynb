{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "#from nd2reader import ND2Reader\n",
    "import napari\n",
    "import bigfish\n",
    "import bigfish.stack as stack\n",
    "import bigfish.detection as detection\n",
    "import bigfish.multistack as multistack\n",
    "import bigfish.plot as plot\n",
    "import time\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "from skimage import io\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1 = \"./\" ##Input Directory with the ND2 files\n",
    "dir2 = os.path.join(dir1,'TiffFiles') #New subdirectory where processed files will be saved\n",
    "dir3 = os.path.join(dir2, 'Max_Projections')  # Directory for max projections\n",
    "dir4 = os.path.join(dir3, 'FOVs')  # Directory for individual FOVs\n",
    "dir6 = os.path.join(dir3, \"Combined_Files\") # Create a directory to save combined files if it doesn't exist\n",
    "dir7 = os.path.join(dir3, 'Spots') #Save output spot files\n",
    "\n",
    "# Directories to check and create if they don't exist\n",
    "dirs_to_create = [dir2, dir3, dir4, dir6]\n",
    "\n",
    "# Loop through each directory\n",
    "for directory in dirs_to_create:\n",
    "    # Check if the directory does not exist\n",
    "    if not os.path.isdir(directory):\n",
    "        # If it doesn't, create the directory\n",
    "        os.mkdir(directory)\n",
    "\n",
    "# Define parameters\n",
    "voxelval = 110.3752759382\n",
    "##radiusval = 250.0\n",
    "radiusval = 2*voxelval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files as OME Tiffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All images are converte to .tif files and saved in the TiffFiles folder\n",
    "- Unclear yet how to change the LUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nis2pyr.convertor import convert_nd2_to_pyramidal_ome_tiff\n",
    "\n",
    "files=[]\n",
    "files += [each for each in os.listdir(dir1) if each.endswith('.nd2')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "string = 'Bleach'\n",
    "mask = np.array([string not in s for s in files]) ##Remove images that were used for testing bleaching\n",
    "\n",
    "files = files[mask]\n",
    "\n",
    "\n",
    "# Iterate over each file in the 'files' list (all ND2 files)\n",
    "for fil in files:\n",
    "    # Print the directory and filename being processed\n",
    "    print(dir1+fil)\n",
    "    \n",
    "    # Convert the ND2 file to pyramidal OME-TIFF format\n",
    "    # Specify the input ND2 file path, output OME-TIFF file path,\n",
    "    # and the maximum number of pyramid levels (set to 1 in this case)\n",
    "    convert_nd2_to_pyramidal_ome_tiff(dir1+fil, dir2+'/'+fil.split(\".\")[0]+'.tif',max_levels=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Intensity Projection and Image Distribution according to FOV \n",
    "So all images corresponding to the same FOV but across different cycles are saved in one folder \n",
    "- each image for one round is split by its channels and one channel image is saved\n",
    "- still need to figure out how to manage when there are multiple channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all TIFF files in the input directory and sort them\n",
    "files = []\n",
    "files += [each for each in os.listdir(dir2) if each.endswith('.tif')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "\n",
    "# Define a function to compute maximum intensity projection along the Z-axis\n",
    "def maximum_intensity_projection(image_stack):\n",
    "    # Calculate the maximum intensity projection along the Z-axis (channels are in the first dimension)\n",
    "    mip = np.max(image_stack, axis=0)\n",
    "    return mip\n",
    "\n",
    "# Loop through each TIFF file\n",
    "for fil in files:\n",
    "    # Read the OME-TIFF file\n",
    "    ome_tiff_path = dir2 + \"/\" + fil\n",
    "    image_stack = tiff.imread(ome_tiff_path)\n",
    "    \n",
    "    # Compute the maximum intensity projection\n",
    "    mip = maximum_intensity_projection(image_stack)\n",
    "    \n",
    "    # Save the max projection image\n",
    "    mip_path = dir3 + \"MAX_\" + fil.split(\".\")[0] + \".tif\"\n",
    "    tiff.imsave(mip_path, mip)\n",
    "    \n",
    "    # Extract FOV information from the filename\n",
    "    FOV = fil.split(\"_\")\n",
    "    \n",
    "    # Create a directory for the FOV if it does not exist\n",
    "    dir5 = dir4 + \"/\" + FOV[1].split(\".\")[0] + \"/\"\n",
    "    if not os.path.isdir(dir5):\n",
    "        os.mkdir(dir5)\n",
    "    \n",
    "    # Save each channel of the max projection image as a separate TIFF file\n",
    "    for chan in range(0, mip.shape[0]):\n",
    "        image = mip[chan, :, :]\n",
    "        savename = dir5 + FOV[0] + \"_channel_\" + str(chan + 1) + \".tif\"\n",
    "        tiff.imsave(savename, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Files combining images from single channel and single FOV across all cycles\n",
    "- This helps in easy visualization of the data and can also be used for data analysis? \n",
    "- the files are saved in the Max_Projection/Combined_Files/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of directories in dir3\n",
    "directories = [name for name in os.listdir(dir3) if os.path.isdir(os.path.join(dir3, name))]\n",
    "directories.sort()  # Sort the list of directories\n",
    "\n",
    "# Iterate over each directory\n",
    "for dir in directories:\n",
    "    fils = []\n",
    "    dirs = dir3 + \"/\" + dir  # Full path to the current directory\n",
    "    fils += [each for each in os.listdir(dirs) if each.endswith('.tif')]  # Get list of TIFF files in the directory\n",
    "    fils = np.array(fils)\n",
    "    fils.sort()  # Sort the list of TIFF files\n",
    "    \n",
    "    # Extract channel information from TIFF filenames\n",
    "    chan = np.char.split(fils, sep=\"_\")\n",
    "    chan = pd.DataFrame(np.stack(chan, axis=0))[2]  # Extract the third element (channel) from each filename\n",
    "    chans = pd.unique(chan)  # Get unique channel values\n",
    "    \n",
    "    # Iterate over each unique channel\n",
    "    for c in chans:\n",
    "        index = chan == c  # Get indices of TIFF files corresponding to the current channel\n",
    "        multifils = fils[index]  # Get TIFF files corresponding to the current channel\n",
    "        multichannel_image = np.zeros((len(multifils), image.shape[0], image.shape[1]))  # Initialize array for multichannel image\n",
    "        round = 0\n",
    "        # Iterate over each TIFF file corresponding to the current channel\n",
    "        for m in multifils:\n",
    "            img = tiff.imread(dirs + \"/\" + m)  # Read TIFF file\n",
    "            multichannel_image[round, :, :] = img  # Store image in multichannel array\n",
    "            round = round + 1\n",
    "        savename = dir6 + dir + \"_channel_\" + c  # Define filename for combined multichannel image\n",
    "        tiff.imsave(savename, multichannel_image)  # Save combined multichannel image as TIFF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FISH Spot Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detecting threshold within multiple FOVs for all rounds and channels\n",
    "- This could potentially be used to get a global threshold value\n",
    "- Alternatively can be used to get individual thresholds for each channel and round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Threshold values for each channel\n",
    "thresh1 = 18  # Threshold for Channel 1 (generally Cy7)\n",
    "thresh2 = 18  # Threshold for Channel 2 (generally Cy5)\n",
    "thresh3 = 18  # Threshold for Channel 3 (generally Cy3B)\n",
    "\n",
    "# Number of cycles for each channel\n",
    "chan1round = 8\n",
    "chan2round = 8\n",
    "chan3round = 8\n",
    "\n",
    "# Number of FOVs to use to detect thresholds\n",
    "nfovs = 15\n",
    "\n",
    "# Create directory to save combined files if it doesn't exist\n",
    "\n",
    "# Get list of TIFF files in the combined files directory and sort them\n",
    "files = []\n",
    "files += [each for each in os.listdir(dir6) if each.endswith('.tif')]\n",
    "files = np.array(files)\n",
    "files.sort()\n",
    "\n",
    "# Extract channel information from TIFF filenames\n",
    "chan = np.char.split(files, sep=\"_\")\n",
    "chan = pd.DataFrame(np.stack(chan, axis=0))[2]\n",
    "chans = pd.unique(chan)\n",
    "\n",
    "# Initialize arrays to store threshold values for each channel\n",
    "ths1 = np.zeros((chan1round * nfovs, 1))\n",
    "ths2 = np.zeros((chan2round * nfovs + 1, 1))\n",
    "ths3 = np.zeros((chan3round * nfovs, 1))\n",
    "\n",
    "# Compute spot radius in pixels\n",
    "spot_radius_px = detection.get_object_radius_pixel(\n",
    "                    voxel_size_nm=(voxelval, voxelval), \n",
    "                    object_radius_nm=(radiusval, radiusval), \n",
    "                    ndim=2)\n",
    "\n",
    "# Channel 1 - Cy7\n",
    "e = 0\n",
    "for fil in range(0, nfovs):\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    filename = files[fil * len(chans)]\n",
    "    img = tiff.imread(dir6 + filename)\n",
    "    print(filename)\n",
    "    # Detect spots\n",
    "    for t in range(0, img.shape[0]):\n",
    "        rna = img[t, :, :]\n",
    "        # LoG filter\n",
    "        rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "        # Local maximum detection\n",
    "        mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "        # Thresholding\n",
    "        threshold = detection.automated_threshold_setting(rna_log, mask)\n",
    "        ths1[e] = threshold\n",
    "        e = e + 1\n",
    "print(\"Finished thresholding for channel 1 after %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Channel 2 - Cy5\n",
    "e = 0\n",
    "for fil in range(0, nfovs):\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    filename = files[fil * len(chans) + 1]\n",
    "    img = tiff.imread(dir6 + filename)\n",
    "    print(filename)\n",
    "    # Detect spots\n",
    "    for t in range(0, img.shape[0]):\n",
    "        rna = img[t, :, :]\n",
    "        # LoG filter\n",
    "        rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "        # Local maximum detection\n",
    "        mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "        # Thresholding\n",
    "        threshold = detection.automated_threshold_setting(rna_log, mask)\n",
    "        ths2[e] = threshold\n",
    "        e = e + 1\n",
    "print(\"Finished thresholding for channel 2 after %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Channel 3 - Cy3B\n",
    "e = 0\n",
    "for fil in range(0, nfovs):\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    filename = files[fil * len(chans) + 2]\n",
    "    img = tiff.imread(dir6 + filename)\n",
    "    print(filename)\n",
    "    # Detect spots\n",
    "    for t in range(0, img.shape[0]):\n",
    "        rna = img[t, :, :]\n",
    "        # LoG filter\n",
    "        rna_log = stack.log_filter(rna, sigma=spot_radius_px)\n",
    "        # Local maximum detection\n",
    "        mask = detection.local_maximum_detection(rna_log, min_distance=spot_radius_px)\n",
    "        # Thresholding\n",
    "        threshold = detection.automated_threshold_setting(rna_log, mask)\n",
    "        ths3[e] = threshold\n",
    "        e = e + 1\n",
    "print(\"Finished thresholding for channel 3 after %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying threshold to detect spots and clusters\n",
    "- spots are saved in the Spots folder as FOV##\\_Channel##\\_#.csv\n",
    "- spot clusters are save in the Spots folder as FOV##\\_Channel##\\_spotclusters_#.csv\n",
    "- clusters information is save in the Spots folder as FOV##\\_Channel##\\_clusters\\_#.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate thresholds for each channel\n",
    "thresh1 = 2 * np.median(ths1) + 40  # Threshold for Channel 1\n",
    "thresh2 = 2 * np.median(ths2) + 40  # Threshold for Channel 2\n",
    "thresh3 = 2 * np.median(ths3) + 40  # Threshold for Channel 3\n",
    "threshs = [thresh1, thresh2, thresh3]\n",
    "\n",
    "# Number of rounds for each channel\n",
    "chan1round = 7\n",
    "chan2round = 7\n",
    "chan3round = 6\n",
    "\n",
    "# Define directory paths and get list of TIFF files\n",
    "files = sorted([each for each in os.listdir(dir6) if each.endswith('.tif')])\n",
    "chan = pd.unique(pd.DataFrame(np.stack(np.char.split(files, sep=\"_\"), axis=0))[2])\n",
    "nfovs = int(len(files) / len(chans))\n",
    "\n",
    "# Iterate over each FOV\n",
    "for fil in range(nfovs):\n",
    "    print(\"--- Start %s seconds ---\" % (time.time() - start_time))\n",
    "    # Iterate over each channel\n",
    "    for cc in range(len(chans) - 1):\n",
    "        print(\"Analysing Channel %s\" % (cc))\n",
    "        filename = files[fil * len(chans) + cc]\n",
    "        img = tiff.imread(os.path.join(dir6, filename))\n",
    "        savenamespots = filename.split(\".\")[0] + \".csv\"\n",
    "        savenamespotscl = filename.split(\".\")[0] + \"_spotclusters.csv\"\n",
    "        savenameclusters = filename.split(\".\")[0] + \"_clusters.csv\"\n",
    "        sp = pd.DataFrame()\n",
    "        spcl = pd.DataFrame()\n",
    "        cl = pd.DataFrame()\n",
    "        print(filename)\n",
    "        # Detect spots and clusters for each round\n",
    "        for t in range(img.shape[0] - 1):\n",
    "            print(\"Analysing Round %s\" % (t + 1))\n",
    "            rna = img[t + 1, :, :]\n",
    "            spots = detection.detect_spots(\n",
    "                images=rna,\n",
    "                return_threshold=False,\n",
    "                threshold=threshs[cc],\n",
    "                voxel_size=(voxelval, voxelval),\n",
    "                spot_radius=(radiusval, radiusval)\n",
    "            )\n",
    "            spots_post_decomposition, dense_regions, reference_spot = detection.decompose_dense(\n",
    "                image=np.uint16(rna),\n",
    "                spots=spots,\n",
    "                voxel_size=(voxelval, voxelval),\n",
    "                spot_radius=(radiusval, radiusval),\n",
    "                alpha=0.75,\n",
    "                beta=0.9,\n",
    "                gamma=15\n",
    "            )\n",
    "            spots_post_clustering, clusters = detection.detect_clusters(\n",
    "                spots=spots_post_decomposition,\n",
    "                voxel_size=(int(voxelval), int(voxelval)),\n",
    "                radius=int(radiusval),\n",
    "                nb_min_spots=4\n",
    "            )\n",
    "            spotspd = pd.DataFrame(spots)\n",
    "            spclpd = pd.DataFrame(spots_post_clustering)\n",
    "            clupd = pd.DataFrame(clusters)\n",
    "            spotspd['round'] = t\n",
    "            spclpd['round'] = t\n",
    "            clupd['round'] = t\n",
    "            sp = pd.concat([sp, spotspd])\n",
    "            spcl = pd.concat([spcl, spclpd])\n",
    "            cl = pd.concat([cl, clupd])\n",
    "        sp.columns = ['Y', 'X', 'round']\n",
    "        cl.columns = ['Y', 'X', 'nspots', 'index', 'round']\n",
    "        spcl.columns = ['Y', 'X', 'clusterindex', 'round']\n",
    "        sp.to_csv(os.path.join(dir6, savenamespots), index=False)\n",
    "        spcl.to_csv(os.path.join(dir6, savenamespotscl), index=False)\n",
    "        cl.to_csv(os.path.join(dir6, savenameclusters), index=False)\n",
    "\n",
    "print(\"Finished thresholding for channel 1 image %s after %s seconds ---\" % (e, time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Napari viewer\n",
    "- View all images in one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the nearest square number below a given limit\n",
    "def nearest_square(limit):\n",
    "    answer = 0\n",
    "    while (answer + 1) ** 2 < limit:\n",
    "        answer += 1\n",
    "    if answer ** 2 == limit:\n",
    "        return answer\n",
    "    else:\n",
    "        return answer + 1\n",
    "\n",
    "# Initialize Napari viewer\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "# Define parameters\n",
    "gap = 2800  # Gap between images\n",
    "totalarea = nearest_square(len(files) / 4)  # Total area for arranging images\n",
    "image = np.zeros((gap * totalarea, gap * totalarea), dtype=np.int16)  # Initialize an empty image array\n",
    "xx = 0\n",
    "yy = 0\n",
    "\n",
    "# Get list of TIFF files\n",
    "files = sorted([each for each in os.listdir(dir6) if each.endswith('.tif')])\n",
    "chan = pd.unique(pd.DataFrame(np.stack(np.char.split(files, sep=\"_\"), axis=0))[2])\n",
    "nfovs = int(len(files) / len(chans))\n",
    "\n",
    "# Iterate over each FOV\n",
    "for fil in range(int(len(files) / 4)):\n",
    "    imagename = files[len(chans) * fil + 3]\n",
    "    print(imagename)\n",
    "    imageloc = os.path.join(dir6, imagename)\n",
    "    im2 = tiff.imread(imageloc)\n",
    "    image[xx * gap:xx * gap + im2.shape[1], yy * gap:yy * gap + im2.shape[1]] = im2[0, :, :]\n",
    "\n",
    "    # Read spot files for each channel\n",
    "    cy7spots = pd.read_csv(os.path.join(dir7, files[len(chans) * fil].split(\".\")[0] + \"_spotclustters.csv\"))\n",
    "    cy5spots = pd.read_csv(os.path.join(dir7, files[len(chans) * fil + 1].split(\".\")[0] + \"_spotclustters.csv\"))\n",
    "    cy3spots = pd.read_csv(os.path.join(dir7, files[len(chans) * fil + 2].split(\".\")[0] + \"_spotclustters.csv\"))\n",
    "\n",
    "    # Update spot coordinates for each channel\n",
    "    cy7spots['Y'] = cy7spots['Y'] + xx * gap\n",
    "    cy7spots['X'] = cy7spots['X'] + yy * gap\n",
    "    cy5spots['Y'] = cy5spots['Y'] + xx * gap\n",
    "    cy5spots['X'] = cy5spots['X'] + yy * gap\n",
    "    cy3spots['Y'] = cy3spots['Y'] + xx * gap\n",
    "    cy3spots['X'] = cy3spots['X'] + yy * gap\n",
    "\n",
    "    # Get gene names for each channel\n",
    "    gency7ind = np.array(cy7spots['round'].values)\n",
    "    genecy7 = [genescy7[j] for j in gency7ind]\n",
    "    gency5ind = np.array(cy5spots['round'].values)\n",
    "    genecy5 = [genescy5[j] for j in gency5ind]\n",
    "    gency3ind = np.array(cy3spots['round'].values)\n",
    "    genecy3 = [genescy3[j] for j in gency3ind]\n",
    "\n",
    "    # Assign gene names to each spot\n",
    "    cy7spots['gene'] = genecy7\n",
    "    cy5spots['gene'] = genecy5\n",
    "    cy3spots['gene'] = genecy3\n",
    "\n",
    "    # Concatenate spot data for each channel\n",
    "    spotscy7 = pd.concat([spotscy7, cy7spots])\n",
    "    spotscy5 = pd.concat([spotscy5, cy5spots])\n",
    "    spotscy3 = pd.concat([spotscy3, cy3spots])\n",
    "\n",
    "    xx = xx + 1\n",
    "    if xx > totalarea - 1:\n",
    "        xx = 0\n",
    "        yy = yy + 1\n",
    "\n",
    "# Add image to Napari viewer\n",
    "imagelayer = viewer.add_image(np.uint16(image))\n",
    "imagelayer.contrast_limits = (0, 65000)\n",
    "\n",
    "# Concatenate spot data for all channels\n",
    "allspots = pd.concat([spotscy7, spotscy5], axis=0)\n",
    "allspots = pd.concat([allspots, spotscy3], axis=0)\n",
    "\n",
    "# Iterate over each gene and add points to the viewer for each channel\n",
    "for round in range(0, len(genescy7)):\n",
    "    cy7 = spotscy7[spotscy7['round'] == round]\n",
    "    cy5 = spotscy5[spotscy5['round'] == round]\n",
    "    cy3 = spotscy3[spotscy3['round'] == round]\n",
    "\n",
    "    viewer.add_points(np.array(cy7)[:, 0:2],\n",
    "                      face_color=color[(len(chans) - 1) * round],\n",
    "                      size=5,\n",
    "                      blending='translucent_no_depth',\n",
    "                      edge_width=0,\n",
    "                      name=genescy7[round])\n",
    "\n",
    "    viewer.add_points(np.array(cy5)[:, 0:2],\n",
    "                      face_color=color[(len(chans) - 1) * round + 1],\n",
    "                      size=5,\n",
    "                      blending='translucent_no_depth',\n",
    "                      edge_width=0,\n",
    "                      name=genescy5[round])\n",
    "\n",
    "    viewer.add_points(np.array(cy3)[:, 0:2],\n",
    "                      face_color=color[(len(chans) - 1) * round + 2],\n",
    "                      size=5,\n",
    "                      blending='translucent_no_depth',\n",
    "                      edge_width=0,\n",
    "                      name=genescy3[round])\n",
    "\n",
    "# Save spot data to CSV files\n",
    "featuresall = allspots['gene']\n",
    "featuresall.to_csv(os.path.join(dir7, 'features.csv'), index=False)\n",
    "allspots.to_csv(os.path.join(dir7, \"allspots.csv\"), index=False)\n",
    "\n",
    "# Add points for all spots to the viewer\n",
    "feat = tuple(np.array(featuresall))\n",
    "pointlayer = viewer.add_points(np.array(allspots)[:, 0:2],\n",
    "                                face_color='white',\n",
    "                                size=5,\n",
    "                                blending='translucent_no_depth',\n",
    "                                edge_width=0,\n",
    "                                name='All_spots',\n",
    "                                opacity=0,\n",
    "                                features=feat)\n",
    "\n",
    "pointlayer.refresh()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('multifish')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd4506f47e7a8e319038edbecbacc3135e1dea1a0fd1c74135d4055cf5626a52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
